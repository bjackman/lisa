{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Various setup, probably mostly unused\n",
    "\n",
    "from env import TestEnv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from conf import LisaLogging\n",
    "from bart.common.Utils import area_under_curve\n",
    "from trappy.plotter import plot_trace\n",
    "from IPython.display import display\n",
    "from trappy import ILinePlot\n",
    "from trappy.stats.grammar import Parser\n",
    "from bart.sched.SchedMultiAssert import SchedMultiAssert\n",
    "import pandas as pd\n",
    "from trace import Trace\n",
    "LisaLogging.setup()\n",
    "import logging\n",
    "logging.getLogger('Trace').setLevel(logging.ERROR)\n",
    "logging.getLogger('Analysis').setLevel(logging.WARNING)\n",
    "logging.getLogger('EnergyMeter').setLevel(logging.DEBUG)\n",
    "%matplotlib inline\n",
    "from platforms.juno_energy import juno_energy\n",
    "from platforms.pixel_energy import pixel_energy\n",
    "import tests.eas.generic\n",
    "from tests.eas.generic import EnergyModelTest\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from wlgen import RTA, Periodic\n",
    "from executor import Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some nonsense to get caiman to work on Brendan's computer\n",
    "p = os.getenv('PATH').split(':')\n",
    "caiman_path = '/opt/ds5_v5.23.0/bin'\n",
    "if caiman_path not in p:\n",
    "    p.insert(0, caiman_path)\n",
    "    os.environ['PATH'] = ':'.join(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from platforms.hikey_energy import hikey_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "te = TestEnv(test_conf={\n",
    "        'modules': ['cgroups'], \n",
    "        'ftrace': {\n",
    "            'events': ['sched_switch', 'cpu_frequency', 'cpu_idle', 'sched_load_avg_cpu', 'sched_load_avg_task', 'irq*']\n",
    "        }\n",
    "    }, force_new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some nonsense to make LISA treat HiKey with the respect it deserves\n",
    "from trappy.stats.Topology import Topology\n",
    "hikey_topology = Topology(clusters=[[0, 1, 2, 3], [4, 5, 6, 7]])\n",
    "te.topology = hikey_topology\n",
    "te.platform['clusters'] = {'big': [0, 1, 2, 3], 'little': [4, 5, 6, 7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spread_evenly(ntasks, ncpus):\n",
    "    ret = [0 for _ in range(ncpus)]\n",
    "    for i in range(ntasks):\n",
    "        ret[i % ncpus] += 1\n",
    "    return ret\n",
    "\n",
    "def get_various_packing_placements(ntasks, ncpus):\n",
    "    max_spread = spread_evenly(ntasks, ncpus)\n",
    "    \n",
    "    max_pack = [ntasks] + [0] * (ncpus - 1)\n",
    "    \n",
    "    half_pack = [ntasks / 2] + spread_evenly(ntasks - (ntasks / 2), ncpus -1)\n",
    "    \n",
    "    return [max_spread, half_pack, max_pack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_everything(nrg_model, cpu=0, task_pct=4):\n",
    "    cpu_node = nrg_model.cpu_nodes[cpu]\n",
    "    cluster_node = cpu_node.parent\n",
    "    ncpus = len(cluster_node.cpus)\n",
    "    min_cap = cpu_node.min_capacity\n",
    "    task_cap = int(1024 * (task_pct / 100.))\n",
    "    \n",
    "    tasks_per_cpu = int(min_cap / task_cap)\n",
    "    ntasks = tasks_per_cpu * ncpus\n",
    "    task_distribs = get_various_packing_placements(ntasks, ncpus)\n",
    "    \n",
    "    util_distribs = []\n",
    "    wloads = []\n",
    "    for task_distrib in task_distribs:\n",
    "        util_distrib = [0 for _ in range(len(nrg_model.cpus))]\n",
    "        for i, num_tasks_on_cpu in enumerate(task_distrib):\n",
    "            util_distrib[cluster_node.cpus[i]] = num_tasks_on_cpu * task_cap\n",
    "        util_distribs.append(util_distrib)\n",
    "\n",
    "    # Stagger the workloads so that they prevent shared idle states\n",
    "    period_s = 10e-3\n",
    "    stagger_s = 0.7e-3\n",
    "    delays = np.arange(period_s, step=stagger_s)\n",
    "    \n",
    "    if len(delays) > ntasks:\n",
    "        print \"WARNING: not enough tasks to cover period with wakeups\"\n",
    "    \n",
    "    tasks = {}\n",
    "    for i in range(ntasks):\n",
    "        tasks['{}pct_{}'.format(task_pct, i)] = {\n",
    "            'kind': 'Periodic',\n",
    "            'params': {\n",
    "                'duty_cycle_pct': task_pct,\n",
    "                'period_ms' : period_s * 1e3 + (i - (ntasks / 2)),\n",
    "                'delay_s' : delays[i % len(delays)],\n",
    "                'duration_s': 2\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    wloads = {\n",
    "        'forced_pack': {\n",
    "            'type': 'rt-app',\n",
    "            'conf': {\n",
    "                'class': 'profile',\n",
    "                'params': tasks,\n",
    "                'cpus': [0, 1, 2, 3]\n",
    "            }\n",
    "        },\n",
    "        'any_cpu': {\n",
    "            'type': 'rt-app',\n",
    "            'conf': {\n",
    "                'class': 'profile',\n",
    "                'params': tasks,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    return task_distribs, util_distribs, wloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_distribs, util_distribs, wloads = get_everything(hikey_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poor_mans_powersave = { # No powersave governor in hikey_defconfi\n",
    "    'governor': 'userspace',\n",
    "    'freqs':{\n",
    "        0: 208000,\n",
    "    }\n",
    "}\n",
    "performance = {\n",
    "    'governor': 'performance'\n",
    "}\n",
    "executor = Executor(te, {\n",
    "        'confs': [{\n",
    "                'tag': 'myconf',\n",
    "                'flags': ['ftrace', 'freeze_userspace'],\n",
    "                'cpufreq': performance,\n",
    "            }],\n",
    "        'wloads': wloads,\n",
    "        'iterations': 5\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "executor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [Trace(te.platform, e.out_dir, ['sched_switch', 'cpu_idle', 'cpu_frequency']) for e in executor.experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trace in traces:\n",
    "    plot_trace(trace.ftrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = [[0, 1, 2, 3], [4, 5, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trace in traces:\n",
    "    ca_signals = []\n",
    "    for cluster in clusters:\n",
    "        ca_signals.append(pd.DataFrame(trace.getClusterActiveSignal(cluster)))\n",
    "    ILinePlot(ca_signals, column=0, drawstyle='steps-post', fill=True, fill_alpha=0.8).view() \n",
    "    trace.analysis.idle.plotClusterIdleStateResidency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def examine_experiment(experiment):\n",
    "    trace = Trace(te.platform, experiment.out_dir, ['sched_switch'])\n",
    "    ma = SchedMultiAssert(trace.ftrace, hikey_topology, experiment.wload.tasks.keys())\n",
    "    cluster_conclusions = []\n",
    "    for cluster in te.topology.get_level('cluster'):\n",
    "        residencies = [t['residency'] for p, t in ma.getResidency('cluster', cluster, percent=True).iteritems()]\n",
    "        if all(r > 90 for r in residencies):\n",
    "            cluster_conclusions.append('PACKED')\n",
    "        else:\n",
    "            cluster_conclusions.append(None)\n",
    "    packed = [i for i, c in enumerate(cluster_conclusions) if c == 'PACKED']\n",
    "    with open(os.path.join(experiment.out_dir, 'energy.json')) as f:\n",
    "        energy = json.load(f)\n",
    "        energy = energy['BAT']\n",
    "    if not packed:\n",
    "        print \"I don't think we packed onto any cluster\"\n",
    "        return False, energy\n",
    "        # plot_trace(trace.ftrace)\n",
    "    elif len(packed) == 1:\n",
    "        [i] = packed\n",
    "        print 'I think we packed onto cluster {} ({})'.format(i, te.topology.get_node('cluster', i))\n",
    "        return True, energy\n",
    "    else:\n",
    "        # this code is\n",
    "        print 'what is this'\n",
    "        # who even.. who wrote this shit\n",
    "        print 'i dont even'\n",
    "        # Microsoft recommends Windows 10 with Microsoft EdgeÂ® for a more secure browsing experience\n",
    "        raise Exception(RuntimeError(GenericBamboozlementComplaintFactory2().generateComplaint('bamboozled!!!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "previous_energy_records = [[], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "energy_records = previous_energy_records\n",
    "for experiment in executor.experiments:\n",
    "    packed, energy = examine_experiment(experiment)\n",
    "    energy_records[packed].append(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(energy_records[0]), np.std(energy_records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(energy_records[1]), np.std(energy_records[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "energy_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = [Trace(te.platform, e.out_dir, ['sched_switch', 'cpu_frequency']) for e in executor.experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = [208000, 432000, 729000, 960000, 1200000]\n",
    "te.platform['freqs']['big'] = freqs\n",
    "te.platform['freqs']['little'] = freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace = traces[0]\n",
    "trace.analysis.frequency.plotClusterFrequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te.target.big_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace.analysis.tasks.plotTasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "..\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raise Exception('HALT HALT HALT, HALT AT THE GATES OF THE GRAVEYARD OR SURELY PERISH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ðŸ’€ðŸ’€ The code graveyard ðŸ’€ðŸ’€\n",
    "*abandon hope all ye who scroll past here, for here is the threshold of the graveyard, whither is sent the code of yore, remembered but not missed, respected but not loved, written by the hand of fearful men and trialled by the silicon of scorned machines*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_task_distribs(nrg_model, cpu=0, task_pct=4):\n",
    "    cpu_node = nrg_model.cpu_nodes[cpu]\n",
    "    cluster_node = cpu_node.parent\n",
    "    ncpus = len(cluster_node.cpus)\n",
    "    min_cap = cpu_node.min_capacity\n",
    "    \n",
    "    task_cap = int(1024 * (task_pct / 100.))\n",
    "    \n",
    "    tasks_per_cpu = int(min_cap / task_cap)\n",
    "    task_distribs = get_various_packing_placements(tasks_per_cpu * ncpus, ncpus)\n",
    "    return task_distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_util_distribs(nrg_model, task_distribs=None, cpu=0, task_pct=4):\n",
    "    if task_distribs == None:\n",
    "        task_distribs = get_task_distribs(nrg_model, cpu, task_pct)\n",
    "    util_distribs = []\n",
    "    \n",
    "    task_cap = int(1024 * (task_pct / 100.))\n",
    "    cpu_node = nrg_model.cpu_nodes[cpu]\n",
    "    cluster_node = cpu_node.parent\n",
    "    \n",
    "    for task_distrib in task_distribs:\n",
    "        util_distrib = [0 for _ in range(len(nrg_model.cpus))]\n",
    "        for i, num_tasks_on_cpu in enumerate(task_distrib):\n",
    "            util_distrib[cluster_node.cpus[i]] = num_tasks_on_cpu * task_cap\n",
    "        util_distribs.append(util_distrib)\n",
    "    return util_distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wloads(nrg_model, task_distribs=None, cpu=0, task_pct=4):\n",
    "    if task_distribs == None:\n",
    "        task_distribs = get_task_distribs(nrg_model, cpu, task_pct)\n",
    "    for task_distrib in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "util_distribs = get_util_distribs(hikey_energy)\n",
    "for ud in util_distribs:\n",
    "    display(hikey_energy.estimate_from_cpu_util(ud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hikey_energy.cpu_nodes[0].min_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_count_df(experiment, clusters, trace=None):\n",
    "    if trace is None:\n",
    "        trace = Trace(te.platform, experiment.out_dir, ['sched_switch'])\n",
    "    tasks = experiment.wload.tasks.keys()\n",
    "    pids = [trace.getTaskByName(t)[0] for t in tasks]\n",
    "    task_cpus = trace.data_frame.task_cpus()[pids]\n",
    "    # drop consecutive duplicates\n",
    "    task_cpus = task_cpus[(task_cpus.shift(+1) != task_cpus).any(axis=1)]\n",
    "\n",
    "    df = pd.DataFrame(columns=[str(c) for c in clusters])\n",
    "    \n",
    "    for (time, row_in) in task_cpus.iterrows():\n",
    "        cluster_task_counts = [0 for c in clusters]\n",
    "        for task_cpu in row_in:\n",
    "            if isnan(task_cpu):\n",
    "                continue\n",
    "            for i, cluster in enumerate(clusters):\n",
    "                if int(task_cpu) in cluster:\n",
    "                    cluster_task_counts[i] += 1\n",
    "                    break\n",
    "    \n",
    "        df.loc[time] = pd.Series(cluster_task_counts, index=df.columns)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def were_tasks_cluster_packed(experiment, clusters, trace=None):\n",
    "    if trace is None:\n",
    "        trace = Trace(te.platform, experiment.out_dir, ['sched_switch'])\n",
    "    cluster_counts = get_cluster_count_df(experiment, clusters, trace)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nrg(model, util):\n",
    "    nrg = model.estimate_from_cpu_util(util, zero_idle=True)\n",
    "    return sum(nrg.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
